"""OpenAI Realtime APIë¥¼ ì‚¬ìš©í•œ ì–‘ë°©í–¥ ìŒì„± ëŒ€í™”"""

from __future__ import annotations

import asyncio
import base64
import json
import os
from typing import Callable, Optional

import streamlit as st

try:
    import websockets
    from websockets.client import WebSocketClientProtocol
except ImportError:
    websockets = None
    WebSocketClientProtocol = None


class RealtimeVoiceChat:
    """OpenAI Realtime APIë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Args:
            api_key: OpenAI API í‚¤. Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´
        """
        if not websockets:
            raise RuntimeError("websockets íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install websockets`ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise RuntimeError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.ws: Optional[WebSocketClientProtocol] = None
        self.model = "gpt-4o-realtime-preview-2024-12-17"
        self.voice = os.getenv("OPENAI_REALTIME_VOICE", "shimmer")
        self.ws_url = "wss://api.openai.com/v1/realtime"
        
    async def connect(self) -> None:
        """Realtime API ì›¹ì†Œì¼“ì— ì—°ê²°"""
        url = f"{self.ws_url}?model={self.model}"
        
        # websockets.connectëŠ” additional_headers íŒŒë¼ë¯¸í„° ì‚¬ìš©
        self.ws = await websockets.connect(
            url,
            additional_headers={
                "Authorization": f"Bearer {self.api_key}",
                "OpenAI-Beta": "realtime=v1"
            }
        )
        
        # ì„¸ì…˜ ì„¤ì •
        await self._configure_session()
    
    async def _configure_session(self) -> None:
        """ì„¸ì…˜ ì„¤ì • ì—…ë°ì´íŠ¸"""
        config = {
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "instructions": (
                    "ë‹¹ì‹ ì€ FRYND í•­ê³µ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤. "
                    "í•­ê³µê¶Œ ê²€ìƒ‰, ê¸°ë‚´ì‹ ì •ë³´, FAQì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
                    "í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”."
                ),
                "voice": self.voice,
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {
                    "model": "whisper-1"
                },
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 500
                },
                "temperature": 0.8,
                "max_response_output_tokens": 4096
            }
        }
        
        await self.ws.send(json.dumps(config))
    
    async def send_text(self, text: str) -> None:
        """í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡"""
        if not self.ws:
            raise RuntimeError("ì›¹ì†Œì¼“ì´ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        message = {
            "type": "conversation.item.create",
            "item": {
                "type": "message",
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": text
                    }
                ]
            }
        }
        
        await self.ws.send(json.dumps(message))
        
        # ì‘ë‹µ ìƒì„± ìš”ì²­
        await self.ws.send(json.dumps({"type": "response.create"}))
    
    async def send_audio(self, audio_bytes: bytes) -> None:
        """ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì†¡ (PCM16 í˜•ì‹)"""
        if not self.ws:
            raise RuntimeError("ì›¹ì†Œì¼“ì´ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # base64 ì¸ì½”ë”©
        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
        
        message = {
            "type": "input_audio_buffer.append",
            "audio": audio_base64
        }
        
        await self.ws.send(json.dumps(message))
    
    async def commit_audio(self) -> None:
        """ì˜¤ë””ì˜¤ ì…ë ¥ ì™„ë£Œ ë° ì‘ë‹µ ìƒì„± ìš”ì²­"""
        if not self.ws:
            raise RuntimeError("ì›¹ì†Œì¼“ì´ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì˜¤ë””ì˜¤ ì…ë ¥ ì»¤ë°‹
        await self.ws.send(json.dumps({"type": "input_audio_buffer.commit"}))
        
        # ì‘ë‹µ ìƒì„± ìš”ì²­
        await self.ws.send(json.dumps({"type": "response.create"}))
    
    async def listen(
        self,
        on_audio: Optional[Callable[[bytes], None]] = None,
        on_text: Optional[Callable[[str], None]] = None,
        on_transcript: Optional[Callable[[str], None]] = None,
        on_error: Optional[Callable[[str], None]] = None
    ) -> None:
        """
        ì›¹ì†Œì¼“ ë©”ì‹œì§€ ìˆ˜ì‹  ë° ì²˜ë¦¬
        
        Args:
            on_audio: ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹  ì‹œ í˜¸ì¶œë  ì½œë°±
            on_text: í…ìŠ¤íŠ¸ ì‘ë‹µ ìˆ˜ì‹  ì‹œ í˜¸ì¶œë  ì½œë°±
            on_transcript: ìŒì„± ì¸ì‹ ê²°ê³¼ ìˆ˜ì‹  ì‹œ í˜¸ì¶œë  ì½œë°±
            on_error: ì—ëŸ¬ ë°œìƒ ì‹œ í˜¸ì¶œë  ì½œë°±
        """
        if not self.ws:
            raise RuntimeError("ì›¹ì†Œì¼“ì´ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            async for message in self.ws:
                try:
                    data = json.loads(message)
                    event_type = data.get("type", "")
                    
                    # ì˜¤ë””ì˜¤ ì‘ë‹µ
                    if event_type == "response.audio.delta":
                        audio_base64 = data.get("delta", "")
                        if audio_base64 and on_audio:
                            audio_bytes = base64.b64decode(audio_base64)
                            on_audio(audio_bytes)
                    
                    # í…ìŠ¤íŠ¸ ì‘ë‹µ
                    elif event_type == "response.text.delta":
                        text = data.get("delta", "")
                        if text and on_text:
                            on_text(text)
                    
                    # ìŒì„± ì¸ì‹ ê²°ê³¼
                    elif event_type == "conversation.item.input_audio_transcription.completed":
                        transcript = data.get("transcript", "")
                        if transcript and on_transcript:
                            on_transcript(transcript)
                    
                    # ì—ëŸ¬ ì²˜ë¦¬
                    elif event_type == "error":
                        error_msg = data.get("error", {}).get("message", "Unknown error")
                        if on_error:
                            on_error(error_msg)
                
                except json.JSONDecodeError:
                    if on_error:
                        on_error("Invalid JSON received")
                except Exception as e:
                    if on_error:
                        on_error(str(e))
        
        except websockets.exceptions.ConnectionClosed:
            if on_error:
                on_error("Connection closed")
    
    async def close(self) -> None:
        """ì›¹ì†Œì¼“ ì—°ê²° ì¢…ë£Œ"""
        if self.ws:
            await self.ws.close()
            self.ws = None


def render_realtime_voice_ui() -> None:
    """Streamlit UIì—ì„œ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§"""
    st.header("ğŸ™ï¸ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”")
    st.caption("í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•˜ê±°ë‚˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. AIê°€ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "realtime_messages" not in st.session_state:
        st.session_state.realtime_messages = []
    
    # ëŒ€í™” ë‚´ì—­ í‘œì‹œ
    for msg in st.session_state.realtime_messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])
            if msg.get("audio"):
                st.audio(msg["audio"], format="audio/wav")
    
    # ì…ë ¥ ë°©ì‹ ì„ íƒ
    col1, col2 = st.columns([1, 5])
    
    with col1:
        input_mode = st.selectbox(
            "ì…ë ¥ ë°©ì‹",
            ["í…ìŠ¤íŠ¸", "ì˜¤ë””ì˜¤ íŒŒì¼"],
            label_visibility="collapsed"
        )
    
    with col2:
        if input_mode == "í…ìŠ¤íŠ¸":
            user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
            if user_input:
                _handle_text_input(user_input)
        else:
            audio_file = st.file_uploader(
                "ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (mp3, wav, m4a ë“±)",
                type=["mp3", "wav", "m4a", "ogg", "webm"],
                key="audio_upload"
            )
            if audio_file:
                if st.button("ğŸ¤ ìŒì„± ì²˜ë¦¬", type="primary"):
                    _handle_audio_file(audio_file)


def _handle_text_input(text: str) -> None:
    """í…ìŠ¤íŠ¸ ì…ë ¥ ì²˜ë¦¬"""
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.realtime_messages.append({
        "role": "user",
        "content": text
    })
    
    # TODO: Realtime API í˜¸ì¶œ
    with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
        try:
            # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            asyncio.run(_send_and_receive(text))
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def _handle_voice_input() -> None:
    """ìŒì„± ì…ë ¥ ì²˜ë¦¬"""
    st.info("ìŒì„± ì…ë ¥ ê¸°ëŠ¥ì€ í˜„ì¬ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤.")
    # TODO: ë§ˆì´í¬ì—ì„œ ì˜¤ë””ì˜¤ ë…¹ìŒ ë° Realtime API ì „ì†¡


def _handle_audio_file(audio_file) -> None:
    """ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
    try:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.audio(audio_file, format=f"audio/{audio_file.type.split('/')[-1]}")
            st.caption("ğŸ¤ ìŒì„± ë©”ì‹œì§€")
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
        audio_bytes = audio_file.read()
        
        # TODO: ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Whisper API ì‚¬ìš© ê°€ëŠ¥)
        # í˜„ì¬ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        with st.spinner("ìŒì„±ì„ ì¸ì‹í•˜ê³  ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            asyncio.run(_send_audio_and_receive(audio_bytes))
        
        st.rerun()
    
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


async def _send_audio_and_receive(audio_bytes: bytes) -> None:
    """ì˜¤ë””ì˜¤ ì „ì†¡ ë° ì‘ë‹µ ìˆ˜ì‹ """
    chat = RealtimeVoiceChat()
    
    try:
        await chat.connect()
        
        # ì‘ë‹µ ìˆ˜ì§‘ìš©
        response_text = ""
        audio_chunks = []
        transcript = ""
        
        def on_text(delta: str):
            nonlocal response_text
            response_text += delta
        
        def on_audio(audio_data: bytes):
            audio_chunks.append(audio_data)
        
        def on_transcript(text: str):
            nonlocal transcript
            transcript = text
        
        def on_error(error_msg: str):
            st.error(f"ì˜¤ë¥˜: {error_msg}")
        
        # TODO: ì˜¤ë””ì˜¤ë¥¼ ì ì ˆí•œ í˜•ì‹(PCM16)ìœ¼ë¡œ ë³€í™˜
        # í˜„ì¬ëŠ” ê°„ë‹¨í•˜ê²Œ Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì „ì†¡
        
        # OpenAI Whisperë¡œ ìŒì„± ì¸ì‹
        from openai import OpenAI
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            tmp_file.write(audio_bytes)
            tmp_path = tmp_file.name
        
        try:
            with open(tmp_path, "rb") as audio_file:
                transcription = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ko"
                )
            
            user_text = transcription.text
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.realtime_messages.append({
                "role": "user",
                "content": f"ğŸ¤ {user_text}"
            })
            
            # í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µ ìƒì„±
            await chat.send_text(user_text)
            
            # ì‘ë‹µ ìˆ˜ì‹  (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            try:
                await asyncio.wait_for(
                    chat.listen(
                        on_text=on_text,
                        on_audio=on_audio,
                        on_transcript=on_transcript,
                        on_error=on_error
                    ),
                    timeout=30.0
                )
            except asyncio.TimeoutError:
                pass
            
            # ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            if response_text:
                audio_data = b"".join(audio_chunks) if audio_chunks else None
                st.session_state.realtime_messages.append({
                    "role": "assistant",
                    "content": response_text,
                    "audio": audio_data
                })
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            import os as os_module
            if os_module.path.exists(tmp_path):
                os_module.remove(tmp_path)
    
    finally:
        await chat.close()


async def _send_and_receive(text: str) -> None:
    """ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ìˆ˜ì‹ """
    chat = RealtimeVoiceChat()
    
    try:
        await chat.connect()
        
        # ì‘ë‹µ ìˆ˜ì§‘ìš©
        response_text = ""
        audio_chunks = []
        
        def on_text(delta: str):
            nonlocal response_text
            response_text += delta
        
        def on_audio(audio_bytes: bytes):
            audio_chunks.append(audio_bytes)
        
        def on_error(error_msg: str):
            st.error(f"ì˜¤ë¥˜: {error_msg}")
        
        # í…ìŠ¤íŠ¸ ì „ì†¡
        await chat.send_text(text)
        
        # ì‘ë‹µ ìˆ˜ì‹  (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
        try:
            await asyncio.wait_for(
                chat.listen(
                    on_text=on_text,
                    on_audio=on_audio,
                    on_error=on_error
                ),
                timeout=30.0
            )
        except asyncio.TimeoutError:
            pass
        
        # ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        if response_text:
            audio_data = b"".join(audio_chunks) if audio_chunks else None
            st.session_state.realtime_messages.append({
                "role": "assistant",
                "content": response_text,
                "audio": audio_data
            })
    
    finally:
        await chat.close()


__all__ = ["RealtimeVoiceChat", "render_realtime_voice_ui"]
